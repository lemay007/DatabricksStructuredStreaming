{"cells":[{"cell_type":"markdown","source":["# Structured Streaming with Azure EventHubs \n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Establish a connection with Event Hubs in Spark\n* Subscribe to and configure an Event Hubs stream\n* Parse JSON records from Event Hubs\n\n## Library Requirements\n\nThe Maven library with coordinate `com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18`\n\n## Resources\n- [Docs for Azure Event Hubs connector](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs)\n- [Documentation on how to install Maven libraries](https://docs.azuredatabricks.net/user-guide/libraries.html#maven-or-spark-package)\n- [Spark-EventHub debugging FAQ](https://github.com/Azure/azure-event-hubs-spark/blob/master/FAQ.md)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3bdce63-c791-4a97-8c57-0aa469386c44"}}},{"cell_type":"markdown","source":["## Lab Setup\n\nTo use this notebook in your own Databricks environment, you will need to create libraries, using the [Create Library](https://docs.azuredatabricks.net/user-guide/libraries.html) interface in Azure Databricks. Follow the steps below to attach the `azure-eventhubs-spark` library to your cluster:\n\n1. In the left-hand navigation menu of your Databricks workspace, select **Clusters**, then select your cluster in the list. If it's not running, start it now.\n\n  ![Select cluster](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/select-cluster.png)\n\n2. Select the **Libraries** tab (1), then select **Install New** (2). In the Install Library dialog, select **Maven** under Library Source (3). Under Coordinates, paste **com.microsoft.azure:azure-eventhubs-spark_2.12:2.3.18** (4), then select **Install**.\n  \n  ![Databricks new Maven library](https://raw.githubusercontent.com/MicrosoftDocs/mslearn_databricks/main/images/install-eventhubs-spark-library.png)\n\n3. Wait until the library successfully installs before continuing.\n\n  ![Library installed](https://databricksdemostore.blob.core.windows.net/images/10-de-learning-path/eventhubs-spark-library-installed.png)\n\nOnce complete, return to this notebook to continue with the lesson."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2192d1c4-dd57-4bd6-a34c-f9c9100769e9"}}},{"cell_type":"markdown","source":["### Getting Started\n\nRun the following cell to configure our classroom and set up a local streaming file read that we'll be writing to Event Hubs."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e1ee27e-4299-4082-b6ea-f0a63a9845c1"}}},{"cell_type":"code","source":["%run ./Includes/Streaming-Demo-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c2d7cc49-aade-4d49-acb6-f39748776439"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Azure Event Hubs</h2>\n\nMicrosoft Azure Event Hubs is a fully managed, real-time data ingestion service.\nYou can stream millions of events per second from any source to build dynamic data pipelines and immediately respond to business challenges.\nIt integrates seamlessly with a host of other Azure services.\n\nEvent Hubs can be used in a variety of applications such as\n* Anomaly detection (fraud/outliers)\n* Application logging\n* Analytics pipelines, such as clickstreams\n* Archiving data\n* Transaction processing\n* User telemetry processing\n* Device telemetry streaming\n* <b>Live dashboarding</b>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1c215622-ffa3-40a1-9287-6e157a5196fe"}}},{"cell_type":"markdown","source":["### Define Connection Strings and Create Configuration Object\n\nThis cell uses a connection string to create a simple `EventHubsConf` object, which will be used to connect.\n\nTo run this notebook, you'll need to configure Event Hubs and provide the relavent information in the following format:\n```\nEndpoint=sb://<event_hubs_namespace>.servicebus.windows.net/;SharedAccessKeyName=<key_name>;SharedAccessKey=<signing_key>=;EntityPath=<event_hubs_instance>\n```\n\nNote that during the setup steps prior to this noteobok, you were instructed to copy the `Connect string-primary key`; you will need to append the EntityPath with the name of your Event Hub instance to that copied string to successfully connect."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48746d4d-1af0-4850-8a15-7469e02093f1"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.eventhubs.{EventHubsConf, EventPosition}\n\nval connectionString = \"Endpoint=sb://<event_hubs_namespace>.servicebus.windows.net/;SharedAccessKeyName=<key_name>;SharedAccessKey=<signing_key>=;EntityPath=<event_hubs_instance>\"\n\nval ehWriteConf = EventHubsConf(connectionString)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9debc9d5-b4f1-4166-954b-578dbce17329"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">import org.apache.spark.eventhubs.{EventHubsConf, EventPosition}\nconnectionString: String = Endpoint=sb://&lt;event_hubs_namespace&gt;.servicebus.windows.net/;SharedAccessKeyName=&lt;key_name&gt;;SharedAccessKey=&lt;signing_key&gt;=;EntityPath=&lt;event_hubs_instance&gt;\nehWriteConf: org.apache.spark.eventhubs.EventHubsConf = org.apache.spark.eventhubs.EventHubsConf@3094b959\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.eventhubs.{EventHubsConf, EventPosition}\nconnectionString: String = Endpoint=sb://&lt;event_hubs_namespace&gt;.servicebus.windows.net/;SharedAccessKeyName=&lt;key_name&gt;;SharedAccessKey=&lt;signing_key&gt;=;EntityPath=&lt;event_hubs_instance&gt;\nehWriteConf: org.apache.spark.eventhubs.EventHubsConf = org.apache.spark.eventhubs.EventHubsConf@3094b959\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Write Stream to Event Hub to Produce Stream\n\nBelow, we configure a streaming write to Event Hubs. Refer to the docs for additional ways to [write data to Event Hubs](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#writing-data-to-eventhubs)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1231f73b-8396-491d-98fc-935baf3928d2"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming.Trigger.ProcessingTime\n\nval checkpointPath = userhome + \"/event-hub/write-checkpoint\"\ndbutils.fs.rm(checkpointPath,true)\n\nval activityStreamDF =activityStreamDF\n  .writeStream\n  .format(\"eventhubs\")\n  .outputMode(\"update\")\n  .options(ehWriteConf.toMap)\n  .trigger(ProcessingTime(\"25 seconds\"))\n  .option(\"checkpointLocation\", checkpointPath)\n  .start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e06edbaf-a343-4d96-95ab-0eef856f5169"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"<div class=\"ansiout\">command-3128071454632820:6: error: recursive value activityStreamDF needs type\nval activityStreamDF =activityStreamDF\n                      ^\n</div>","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>"]}}],"execution_count":0},{"cell_type":"code","source":["activityStreamDF = spark.writeStream.format(\"eventhubs\").outputMode(\"update\").options(ehWriteConf.toMap).trigger(ProcessingTime(\"25 seconds\")).option(\"checkpointLocation\",checkpointPath).start()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"eb2314fd-f4d3-4ed5-9b03-11e4af2678ff"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2189684857813501&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>activityStreamDF <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;eventhubs&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;update&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>ehWriteConf<span class=\"ansi-blue-fg\">.</span>toMap<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>trigger<span class=\"ansi-blue-fg\">(</span>ProcessingTime<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;25 seconds&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;checkpointLocation&#34;</span><span class=\"ansi-blue-fg\">,</span>checkpointPath<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;SparkSession&#39; object has no attribute &#39;writeStream&#39;</div>","errorSummary":"<span class=\"ansi-red-fg\">AttributeError</span>: &#39;SparkSession&#39; object has no attribute &#39;writeStream&#39;","metadata":{},"errorTraceType":"html","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansi-red-fg\">---------------------------------------------------------------------------</span>\n<span class=\"ansi-red-fg\">AttributeError</span>                            Traceback (most recent call last)\n<span class=\"ansi-green-fg\">&lt;command-2189684857813501&gt;</span> in <span class=\"ansi-cyan-fg\">&lt;module&gt;</span>\n<span class=\"ansi-green-fg\">----&gt; 1</span><span class=\"ansi-red-fg\"> </span>activityStreamDF <span class=\"ansi-blue-fg\">=</span> spark<span class=\"ansi-blue-fg\">.</span>writeStream<span class=\"ansi-blue-fg\">.</span>format<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;eventhubs&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>outputMode<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;update&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>options<span class=\"ansi-blue-fg\">(</span>ehWriteConf<span class=\"ansi-blue-fg\">.</span>toMap<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>trigger<span class=\"ansi-blue-fg\">(</span>ProcessingTime<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;25 seconds&#34;</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>option<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">&#34;checkpointLocation&#34;</span><span class=\"ansi-blue-fg\">,</span>checkpointPath<span class=\"ansi-blue-fg\">)</span><span class=\"ansi-blue-fg\">.</span>start<span class=\"ansi-blue-fg\">(</span><span class=\"ansi-blue-fg\">)</span>\n\n<span class=\"ansi-red-fg\">AttributeError</span>: &#39;SparkSession&#39; object has no attribute &#39;writeStream&#39;</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Event Hubs Configuration\n\nAbove, a simple `EventHubsConf` object is used to write data. There are [numerous additional options for configuration](https://github.com/Azure/azure-event-hubs-spark/blob/master/docs/structured-streaming-eventhubs-integration.md#eventhubsconf). Below, we specify an `EventPosition` ([docs](https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/streaming-event-hubs#eventposition)) and limit our throughput by setting `MaxEventsPerTrigger`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"288994cb-b243-4ba9-8f79-dbe47ed15014"}}},{"cell_type":"code","source":["%scala\n\nval eventHubsConf = EventHubsConf(connectionString)\n  .setStartingPosition(EventPosition.fromStartOfStream)\n  .setMaxEventsPerTrigger(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66f7b70d-328f-4ce3-a141-2154ebdfebb1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">eventHubsConf: org.apache.spark.eventhubs.EventHubsConf = org.apache.spark.eventhubs.EventHubsConf@44a10018\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">eventHubsConf: org.apache.spark.eventhubs.EventHubsConf = org.apache.spark.eventhubs.EventHubsConf@44a10018\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### READ Stream using EventHub\n\nThe `readStream` method is a <b>transformation</b> that outputs a DataFrame with specific schema specified by `.schema()`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4c27ed6-8476-4400-82c7-e15a215a6b74"}}},{"cell_type":"code","source":["%scala\n\nspark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n\nval eventStreamDF = spark.readStream\n  .format(\"eventhubs\")\n  .options(eventHubsConf.toMap)\n  .load()\n\neventStreamDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"17f29914-df7c-475c-954b-fbe438516d63"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"eventStreamDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"body","type":"binary","nullable":true,"metadata":{}},{"name":"partition","type":"string","nullable":true,"metadata":{}},{"name":"offset","type":"string","nullable":true,"metadata":{}},{"name":"sequenceNumber","type":"long","nullable":true,"metadata":{}},{"name":"enqueuedTime","type":"timestamp","nullable":true,"metadata":{}},{"name":"publisher","type":"string","nullable":true,"metadata":{}},{"name":"partitionKey","type":"string","nullable":true,"metadata":{}},{"name":"properties","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}},{"name":"systemProperties","type":{"type":"map","keyType":"string","valueType":"string","valueContainsNull":true},"nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- body: binary (nullable = true)\n |-- partition: string (nullable = true)\n |-- offset: string (nullable = true)\n |-- sequenceNumber: long (nullable = true)\n |-- enqueuedTime: timestamp (nullable = true)\n |-- publisher: string (nullable = true)\n |-- partitionKey: string (nullable = true)\n |-- properties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n |-- systemProperties: map (nullable = true)\n |    |-- key: string\n |    |-- value: string (valueContainsNull = true)\n\neventStreamDF: org.apache.spark.sql.DataFrame = [body: binary, partition: string ... 7 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- body: binary (nullable = true)\n-- partition: string (nullable = true)\n-- offset: string (nullable = true)\n-- sequenceNumber: long (nullable = true)\n-- enqueuedTime: timestamp (nullable = true)\n-- publisher: string (nullable = true)\n-- partitionKey: string (nullable = true)\n-- properties: map (nullable = true)\n    |-- key: string\n    |-- value: string (valueContainsNull = true)\n-- systemProperties: map (nullable = true)\n    |-- key: string\n    |-- value: string (valueContainsNull = true)\n\neventStreamDF: org.apache.spark.sql.DataFrame = [body: binary, partition: string ... 7 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Most of the fields in this response are metadata describing the state of the Event Hubs stream. We are specifically interested in the `body` field, which contains our JSON payload.\n\nNoting that it's encoded as binary, as we select it, we'll cast it to a string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"38cd31d5-1bf1-495f-965a-34d5e98373f0"}}},{"cell_type":"code","source":["%scala\nval bodyDF = eventStreamDF.select('body.cast(\"STRING\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94e84ac5-e2dd-44f0-bae2-f0615fc0e289"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"bodyDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"body","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">bodyDF: org.apache.spark.sql.DataFrame = [body: string]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">bodyDF: org.apache.spark.sql.DataFrame = [body: string]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Each line of the streaming data becomes a row in the DataFrame once an <b>action</b> such as `writeStream` is invoked.\n\nNotice that nothing happens until you engage an action, i.e. a `display()` or `writeStream`."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4a2c688d-07c9-4ecf-b976-e659ce321640"}}},{"cell_type":"code","source":["%scala\ndisplay(bodyDF, streamName= \"bodyDF\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e08432f1-f907-40e1-b1e2-9baa46a8012a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"org.apache.spark.eventhubs.ConnectionStringBuilder$IllegalConnectionStringFormatException: Endpoint should be in format sb://&lt;namespaceName&gt;.&lt;domainName&gt;\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.parseConnectionString(ConnectionStringBuilder.scala:352)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.&lt;init&gt;(ConnectionStringBuilder.scala:72)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder$.apply(ConnectionStringBuilder.scala:432)\n\tat org.apache.spark.eventhubs.EventHubsConf.name(EventHubsConf.scala:192)\n\tat org.apache.spark.sql.eventhubs.EventHubsSource.&lt;init&gt;(EventHubsSource.scala:84)\n\tat org.apache.spark.sql.eventhubs.EventHubsSourceProvider.createSource(EventHubsSourceProvider.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSource(DataSource.scala:326)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$1(MicroBatchExecution.scala:98)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:95)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:484)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:484)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:460)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:428)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.planQuery(MicroBatchExecution.scala:93)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:163)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:163)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:353)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:341)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:268)\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: sb://&lt;event_hubs_namespace&gt;.servicebus.windows.net/\n\tat java.net.URI$Parser.fail(URI.java:2873)\n\tat java.net.URI$Parser.parseAuthority(URI.java:3211)\n\tat java.net.URI$Parser.parseHierarchical(URI.java:3122)\n\tat java.net.URI$Parser.parse(URI.java:3078)\n\tat java.net.URI.&lt;init&gt;(URI.java:588)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.parseConnectionString(ConnectionStringBuilder.scala:347)\n\t... 61 more\n","errorSummary":"ERROR: Some streams terminated before this command could finish!","metadata":{},"errorTraceType":"raw","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\norg.apache.spark.eventhubs.ConnectionStringBuilder$IllegalConnectionStringFormatException: Endpoint should be in format sb://&lt;namespaceName&gt;.&lt;domainName&gt;\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.parseConnectionString(ConnectionStringBuilder.scala:352)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.&lt;init&gt;(ConnectionStringBuilder.scala:72)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder$.apply(ConnectionStringBuilder.scala:432)\n\tat org.apache.spark.eventhubs.EventHubsConf.name(EventHubsConf.scala:192)\n\tat org.apache.spark.sql.eventhubs.EventHubsSource.&lt;init&gt;(EventHubsSource.scala:84)\n\tat org.apache.spark.sql.eventhubs.EventHubsSourceProvider.createSource(EventHubsSourceProvider.scala:84)\n\tat org.apache.spark.sql.execution.datasources.DataSource.createSource(DataSource.scala:326)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$1(MicroBatchExecution.scala:98)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:95)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:93)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:484)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:86)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:484)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren(LogicalPlan.scala:196)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryLikeLogicalPlan.mapChildren$(LogicalPlan.scala:195)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.plans.logical.UnaryNode.mapChildren(LogicalPlan.scala:223)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:489)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:262)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:258)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:29)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:460)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:428)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.planQuery(MicroBatchExecution.scala:93)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:163)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:163)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:353)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:854)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:341)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:268)\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: sb://&lt;event_hubs_namespace&gt;.servicebus.windows.net/\n\tat java.net.URI$Parser.fail(URI.java:2873)\n\tat java.net.URI$Parser.parseAuthority(URI.java:3211)\n\tat java.net.URI$Parser.parseHierarchical(URI.java:3122)\n\tat java.net.URI$Parser.parse(URI.java:3078)\n\tat java.net.URI.&lt;init&gt;(URI.java:588)\n\tat org.apache.spark.eventhubs.ConnectionStringBuilder.parseConnectionString(ConnectionStringBuilder.scala:347)\n\t... 61 more"]}}],"execution_count":0},{"cell_type":"markdown","source":["While we can see our JSON data now that it's cast to string type, we can't directly manipulate it.\n\nBefore proceeding, stop this stream. We'll continue building up transformations against this streaming DataFrame, and a new action will trigger an additional stream."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"574baea1-385d-42a2-bd78-f8e435330009"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active if s.name == \"bodyDF\") s.stop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbda01a6-3195-43d3-92ae-776e33d0a984"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## <img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Parse the JSON payload\n\nThe EventHub acts as a sort of \"firehose\" (or asynchronous buffer) and displays raw data in the JSON format.\n\nIf desired, we could save this as raw bytes or strings and parse these records further downstream in our processing.\n\nHere, we'll directly parse our data so we can interact with the fields.\n\nThe first step is to define the schema for the JSON payload.\n\n:SIDENOTE: Both time fields are encoded as `LongType` here because of non-standard formatting."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"70f7f033-7aef-4e7a-ab21-25b62654bc87"}}},{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, DoubleType}\n\nlazy val schema = StructType(List(\n  StructField(\"Arrival_Time\", LongType),\n  StructField(\"Creation_Time\", LongType),\n  StructField(\"Device\", StringType),\n  StructField(\"Index\", LongType),\n  StructField(\"Model\", StringType),\n  StructField(\"User\", StringType),\n  StructField(\"gt\", StringType),\n  StructField(\"x\", DoubleType),\n  StructField(\"y\", DoubleType),\n  StructField(\"z\", DoubleType),\n  StructField(\"geolocation\", StructType(List(\n    StructField(\"PostalCode\", StringType),\n    StructField(\"StateProvince\", StringType),\n    StructField(\"city\", StringType),\n    StructField(\"country\", StringType)))),\n  StructField(\"id\", StringType)))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1f8500e3-078b-4efd-8af0-4c127d9e655b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, DoubleType}\nschema: org.apache.spark.sql.types.StructType = &lt;lazy&gt;\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.types.{StructField, StructType, StringType, LongType, DoubleType}\nschema: org.apache.spark.sql.types.StructType = &lt;lazy&gt;\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Parse the data\n\nNext we can use the function `from_json` to parse out the full message with the schema specified above.\n\nWhen parsing a value from JSON, we end up with a single column containing a complex object."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbc902c9-a8fa-4c33-afdf-90ec12fcf181"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.from_json\n\nval parsedEventsDF = bodyDF.select(\n  from_json('body, schema).alias(\"json\"))\n\nparsedEventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4b49ade5-ad09-4123-9ae1-56f3cf754cf1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"parsedEventsDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"json","type":{"type":"struct","fields":[{"name":"Arrival_Time","type":"long","nullable":true,"metadata":{}},{"name":"Creation_Time","type":"long","nullable":true,"metadata":{}},{"name":"Device","type":"string","nullable":true,"metadata":{}},{"name":"Index","type":"long","nullable":true,"metadata":{}},{"name":"Model","type":"string","nullable":true,"metadata":{}},{"name":"User","type":"string","nullable":true,"metadata":{}},{"name":"gt","type":"string","nullable":true,"metadata":{}},{"name":"x","type":"double","nullable":true,"metadata":{}},{"name":"y","type":"double","nullable":true,"metadata":{}},{"name":"z","type":"double","nullable":true,"metadata":{}},{"name":"geolocation","type":{"type":"struct","fields":[{"name":"PostalCode","type":"string","nullable":true,"metadata":{}},{"name":"StateProvince","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"country","type":"string","nullable":true,"metadata":{}}]},"nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}}]},"nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">root\n |-- json: struct (nullable = true)\n |    |-- Arrival_Time: long (nullable = true)\n |    |-- Creation_Time: long (nullable = true)\n |    |-- Device: string (nullable = true)\n |    |-- Index: long (nullable = true)\n |    |-- Model: string (nullable = true)\n |    |-- User: string (nullable = true)\n |    |-- gt: string (nullable = true)\n |    |-- x: double (nullable = true)\n |    |-- y: double (nullable = true)\n |    |-- z: double (nullable = true)\n |    |-- geolocation: struct (nullable = true)\n |    |    |-- PostalCode: string (nullable = true)\n |    |    |-- StateProvince: string (nullable = true)\n |    |    |-- city: string (nullable = true)\n |    |    |-- country: string (nullable = true)\n |    |-- id: string (nullable = true)\n\nimport org.apache.spark.sql.functions.from_json\nparsedEventsDF: org.apache.spark.sql.DataFrame = [json: struct&lt;Arrival_Time: bigint, Creation_Time: bigint ... 10 more fields&gt;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">root\n-- json: struct (nullable = true)\n    |-- Arrival_Time: long (nullable = true)\n    |-- Creation_Time: long (nullable = true)\n    |-- Device: string (nullable = true)\n    |-- Index: long (nullable = true)\n    |-- Model: string (nullable = true)\n    |-- User: string (nullable = true)\n    |-- gt: string (nullable = true)\n    |-- x: double (nullable = true)\n    |-- y: double (nullable = true)\n    |-- z: double (nullable = true)\n    |-- geolocation: struct (nullable = true)\n    |    |-- PostalCode: string (nullable = true)\n    |    |-- StateProvince: string (nullable = true)\n    |    |-- city: string (nullable = true)\n    |    |-- country: string (nullable = true)\n    |-- id: string (nullable = true)\n\nimport org.apache.spark.sql.functions.from_json\nparsedEventsDF: org.apache.spark.sql.DataFrame = [json: struct&lt;Arrival_Time: bigint, Creation_Time: bigint ... 10 more fields&gt;]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["Note that we can further parse this to flatten the schema entirely and properly cast our time fields."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ed261c8c-91cf-4a03-abff-578b192946b7"}}},{"cell_type":"code","source":["%scala\n\nimport org.apache.spark.sql.functions.{from_unixtime, col}\n\nval flatSchemaDF = parsedEventsDF\n  .select(from_unixtime(col(\"json.Arrival_Time\")/1000).alias(\"Arrival_Time\").cast(\"timestamp\"),\n          (col(\"json.Creation_Time\")/1E9).alias(\"Creation_Time\").cast(\"timestamp\"),\n          col(\"json.Device\").alias(\"Device\"),\n          col(\"json.Index\").alias(\"Index\"),\n          col(\"json.Model\").alias(\"Model\"),\n          col(\"json.User\").alias(\"User\"),\n          col(\"json.gt\").alias(\"gt\"),\n          col(\"json.x\").alias(\"x\"),\n          col(\"json.y\").alias(\"y\"),\n          col(\"json.z\").alias(\"z\"),\n          col(\"json.id\").alias(\"id\"),\n          col(\"json.geolocation.country\").alias(\"country\"),\n          col(\"json.geolocation.city\").alias(\"city\"),\n          col(\"json.geolocation.PostalCode\").alias(\"PostalCode\"),\n          col(\"json.geolocation.StateProvince\").alias(\"StateProvince\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"adaa9f12-4cd0-4d47-bd1e-0435e4ec3336"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[{"name":"flatSchemaDF","typeStr":"org.apache.spark.sql.DataFrame","schema":{"type":"struct","fields":[{"name":"Arrival_Time","type":"timestamp","nullable":true,"metadata":{}},{"name":"Creation_Time","type":"timestamp","nullable":true,"metadata":{}},{"name":"Device","type":"string","nullable":true,"metadata":{}},{"name":"Index","type":"long","nullable":true,"metadata":{}},{"name":"Model","type":"string","nullable":true,"metadata":{}},{"name":"User","type":"string","nullable":true,"metadata":{}},{"name":"gt","type":"string","nullable":true,"metadata":{}},{"name":"x","type":"double","nullable":true,"metadata":{}},{"name":"y","type":"double","nullable":true,"metadata":{}},{"name":"z","type":"double","nullable":true,"metadata":{}},{"name":"id","type":"string","nullable":true,"metadata":{}},{"name":"country","type":"string","nullable":true,"metadata":{}},{"name":"city","type":"string","nullable":true,"metadata":{}},{"name":"PostalCode","type":"string","nullable":true,"metadata":{}},{"name":"StateProvince","type":"string","nullable":true,"metadata":{}}]},"tableIdentifier":null}],"data":"<div class=\"ansiout\">import org.apache.spark.sql.functions.{from_unixtime, col}\nflatSchemaDF: org.apache.spark.sql.DataFrame = [Arrival_Time: timestamp, Creation_Time: timestamp ... 13 more fields]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.sql.functions.{from_unixtime, col}\nflatSchemaDF: org.apache.spark.sql.DataFrame = [Arrival_Time: timestamp, Creation_Time: timestamp ... 13 more fields]\n</div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["This flat schema provides us the ability to view each nested field as a column."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9b985c41-66ec-4422-9122-cc3b37c10361"}}},{"cell_type":"code","source":["%scala\ndisplay(flatSchemaDF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"22f283c4-1b4e-4105-a0bb-c03b04a4e75e"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Stop all active streams"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fbd91863-1215-44eb-9565-e5709ea12cc8"}}},{"cell_type":"code","source":["%scala\nfor (s <- spark.streams.active)\n  s.stop"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97c8f0d5-3d89-4a68-8b60-ad42e7b6d9b4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"3.Streaming-With-Event-Hubs","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3128071454632811}},"nbformat":4,"nbformat_minor":0}
